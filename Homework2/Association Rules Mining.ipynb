{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfpgrowth\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import fp_growth_py3 as fpg\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据预处理\n",
    "从文件中读取数据集，对related_same_month_brand属性进行关联规则挖掘。\n",
    "数据预处理主要是丢弃丢失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefunction(x) :\n",
    "    x = json.loads(x)\n",
    "    if not x :\n",
    "        return np.nan\n",
    "    else :\n",
    "        return x\n",
    "\n",
    "converts = {\n",
    "    \"visitor_home_cbgs\":prefunction,\n",
    "    \"visitor_work_cbgs\":prefunction,\n",
    "    \"related_same_day_brand\":prefunction,\n",
    "    \"related_same_month_brand\":prefunction,\n",
    "    \"top_brands\":prefunction,\n",
    "    \"popularity_by_hour\":prefunction,\n",
    "    \"popularity_by_day\":prefunction\n",
    "}\n",
    "    \n",
    "data = pd.read_csv(\"cbg_patterns.csv\",converters = converts)\n",
    "\n",
    "dataSet = list(data[\"related_same_month_brand\"].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.找出频繁项集\n",
    "利用FP-Growth算法，构造FP-tree，从FP-tree中找到频繁项集。设置最小支持度为0.2\n",
    "\n",
    "输出所有频繁项集，及其支持度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frequent_itemsets = fpg.find_frequent_itemsets(dataSet, minimum_support=0.2*len(dataSet), include_support=True)\n",
    "print(type(frequent_itemsets))   # print type\n",
    "\n",
    "result = []\n",
    "for itemset, support in frequent_itemsets:    # 将generator结果存入list\n",
    "    result.append((itemset, support/len(dataSet)))\n",
    "\n",
    "    \n",
    "result_patterns = [i[0] for i in result]\n",
    "result_support = [i[1] for i in result]\n",
    "patterns_df = pd.DataFrame({\"fluent_patterns\":result_patterns,\"support\":result_support})\n",
    "patterns = {}\n",
    "for i in result :\n",
    "    patterns[frozenset(sorted(i[0]))] = i[1]\n",
    "patterns_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.导出关联规则\n",
    "从FP-tree和频繁项集中导出关联规则，并计算关联规则的置信度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rules(patterns, min_confidence):\n",
    "    patterns_group = group_patterns_by_length(patterns)\n",
    "    raw_rules = defaultdict(set)\n",
    "    for length, pattern_list in patterns_group.items():\n",
    "        if length == 1:\n",
    "            continue\n",
    "        for pattern, support in pattern_list:\n",
    "            item_list = list(pattern)\n",
    "            for window_size in range(1, length):\n",
    "                for i in range(0, length - window_size):\n",
    "                    for j in range(i + window_size, length):\n",
    "                        base_set = frozenset(item_list[i:j])\n",
    "                        predict_set = frozenset(pattern - base_set)\n",
    "                        confidence = support / patterns.get(base_set)\n",
    "                        if confidence > min_confidence:\n",
    "                            raw_rules[base_set].add((predict_set, confidence))\n",
    "\n",
    "                        base_set, predict_set = predict_set, base_set\n",
    "                        confidence = support / patterns.get(base_set)\n",
    "                        if confidence > min_confidence:\n",
    "                            raw_rules[base_set].add((predict_set, confidence))\n",
    "    return raw_rules\n",
    "\n",
    "def group_patterns_by_length(patterns):\n",
    "    result = defaultdict(list)\n",
    "    for pattern, support in patterns.items():\n",
    "        result[len(pattern)].append((pattern, support))\n",
    "    return result\n",
    "\n",
    "def transform(raw_rules):\n",
    "    result = list()\n",
    "    for base_set, predict_set_list in raw_rules.items():\n",
    "        for predict_set, confidence in predict_set_list:\n",
    "            result.append((base_set, predict_set, confidence))\n",
    "    \n",
    "    return result\n",
    "\n",
    "raw_rules = generate_rules(patterns, 0.7)\n",
    "rules = transform(raw_rules)\n",
    "rules.sort(key=lambda x: x[2], reverse=True)\n",
    "rules_a = [i[0] for i in rules]\n",
    "rules_b = [i[1] for i in rules]\n",
    "confidence = [i[2] for i in rules]\n",
    "rules_df = pd.DataFrame({\"rules_a\":rules_a,\"rules_b\":rules_b,\"confidence\":confidence})\n",
    "rules_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.对关联规则进行评价\n",
    "使用Lift和全置信度指标进行评价。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift= []\n",
    "fsupport = []\n",
    "\n",
    "# 计算Lift\n",
    "for index,row in rules_df.iterrows():\n",
    "    temp = patterns[row[\"rules_a\"]|row[\"rules_b\"]] / patterns[row[\"rules_a\"]] / patterns[row[\"rules_b\"]] \n",
    "    lift.append(temp)\n",
    "rules_df[\"lift\"] = lift\n",
    "\n",
    "# 计算kulc\n",
    "for index,row in rules_df.iterrows():\n",
    "    temp = patterns[row[\"rules_a\"]|row[\"rules_b\"]] / patterns[row[\"rules_b\"]]\n",
    "    fsupport.append( (row[\"confidence\"]+temp)/2 )\n",
    "rules_df[\"kulc\"] = fsupport\n",
    "rules_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.可视化展示挖掘结果 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,2,figsize=(20,5))\n",
    "rules_df.plot(kind='hexbin',x=\"confidence\",y=\"lift\",C=\"kulc\",gridsize=10,ax = axes[0],title='Picture 1: evaluation')\n",
    "\n",
    "axes[1].bar([str(i) for i in patterns_df[\"fluent_patterns\"]],patterns_df[\"support\"])\n",
    "axes[1].set_xticklabels([str(i) for i in patterns_df[\"fluent_patterns\"]],rotation=90)\n",
    "axes[1].set_xlabel(\"frequent item set\")\n",
    "axes[1].set_ylabel(\"support\")\n",
    "axes[1].set_title(\"Picture2: support of frequent item set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture1展示了每个频繁规则的置信度、提升度、kulc。一般情况下，我们认为提升度大于3的关联规则是一个比较强的关联规则，当提升度为1时两者没有关联，但本数据集中关联规则的提升度大多在1-1.3，所以是比较弱的关联规则。\n",
    "Picture2展示了所有频繁项集的支持度，可见最受欢迎的品牌是麦当劳，紧随其后的是沃尔玛，同时去麦当劳和沃尔玛的人也很多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.barh([str(i)+\"->\"+str(j) for i,j in zip(rules_df[\"rules_a\"],rules_df[\"rules_b\"])], [i for i in rules_df[\"confidence\"]], height=0.7, align='center', color='#AAAAAA')\n",
    "# ax.set_yticklabels(rules_df[\"confidence\"])      # 也可以在这里设置 条条 的标签~\n",
    "ax.set_xlabel('confidence')\n",
    "ax.set_ylabel('association rules')\n",
    "ax.invert_yaxis()\n",
    "ax.set_title('Picture3: confidence of association rules')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture3展示了所有的关联规则及其对应的置信度，从图中可以看出，所有关联规则的置信度都大于70%。尤其是第一条，去过SUBWAY和walmart的人几乎肯定会去mcdonalds。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
